---
description: 
globs: *.py
alwaysApply: false
---
## Architecture & Code Structure
- **Use the MCP SDK's server builder (FastMCP)** – Initialize a `FastMCP` server instance (e.g. in a main `server.py`) with a clear name for your application. Avoid writing custom networking or JSON handling; let the SDK handle connection management and message routing.
- **Separation of concerns** – Organize code into modules for distinct purposes. Keep MCP interface code (server setup, tool/resource definitions) separate from business logic (strategy algorithms, data loading, backtest engine). For example, have a module for strategy logic and another for MCP server initialization that calls into that logic.
- **Leverage SDK conventions** – Define MCP **tools** and **resources** using the provided decorators (e.g. `@mcp.tool()`, `@mcp.resource()`) instead of manual routing. This ensures the server advertises capabilities correctly to the client and maintains protocol compliance. Use the SDK’s structure (prompts, tools, resources as needed) to align with MCP’s expected architecture.
- **Lifecycle management** – If your server needs setup/teardown (database connections, data pre-loading), use the `lifespan` context provided by FastMCP (or similar startup hooks) rather than running setup code at import time. This yields a cleaner architecture and integrates with MCP’s lifecycle (init and shutdown events) gracefully.
- **Minimal server entry-point** – Keep the code that launches the server lightweight. For example, use an `if __name__ == "__main__":` block or a small CLI to start the MCP server (possibly using `mcp.run()` or `uvicorn` if using HTTP). This makes it easier to start the server in different modes (dev, production) and to test components in isolation.

## Agent Registration & Session Management
- **Explicit agent registration** – Provide tools or functions for registering a new trading strategy/agent into the system (e.g. a `register_strategy(name, config)` tool). When an agent is registered, store its definition (parameters, possibly code or identifier) in an internal registry. Confirm registration by returning a unique ID or name to reference that strategy later.
- **Session isolation** – Design the server to handle multiple sessions or users without interference. Utilize MCP’s session features to keep each session’s data separate. For example, maintain a session-specific structure (dictionary or context object) that holds the strategies and state for that session. **Do not use global state** for per-session data; instead, use the `Context` or session storage provided by the SDK (e.g. `ctx.session` or similar) to tie data to a session.
- **Track session state** – Maintain clear state for each strategy/agent within a session: whether it’s idle, running a backtest, or completed. If a strategy is running, subsequent actions (like checking results) should reference the correct session and strategy. Use unique keys (like session_id + strategy_id) if storing in a global registry to avoid collisions, but prefer built-in session context when available.
- **Cleanup and lifecycle** – Handle the removal or resetting of strategies when appropriate. For example, if a session ends or an agent is unregistered, ensure its state is cleaned up from the registry. This prevents stale data from accumulating and ensures each new session starts fresh or with only intended persisted state.
- **Concurrent execution considerations** – If multiple agents or multiple sessions might run backtests concurrently, ensure that shared resources are managed safely. Use locks or asyncio synchronization for shared data, or better, avoid sharing mutable global data altogether by scoping it per session/agent. This will prevent race conditions when traders run strategies in parallel.

## Strategy Execution
- **Encapsulate backtest logic** – Implement the core backtest runner in a dedicated component (function or class) separate from the MCP interface. The tool that starts a backtest should call this component, passing in the strategy details and data. This separation makes the logic reusable and easier to test (you can unit test the backtest runner without MCP).
- **Asynchronous or background execution** – If a backtest is time-consuming, run it asynchronously to avoid blocking the MCP server event loop. Mark the tool function as `async` and use `await` for the backtest function, or offload heavy computation to a background thread or task. This allows the server to handle other messages (or heartbeats) and even stream progress without freezing.
- **Progress updates** – For long-running strategies, consider providing feedback during execution. For example, periodically log progress or send interim results (if supported by MCP, e.g. via an observable resource or notifications). This keeps the user informed and helps avoid timeouts. Design the backtest runner to yield or callback progress (every N ticks or at major steps) so the AI agent can relay this to the user if needed.
- **Result collection** – Once a strategy run finishes, gather results into a structured format. This could be a dataclass or dict containing summary metrics (e.g. total return, win rate, max drawdown) and possibly detailed logs or trade history. Return this result in the tool’s response or store it in session state for later retrieval via a resource. Ensure the results are easily serializable to JSON (use simple types or lists/dicts for the content).
- **Error handling in execution** – Anticipate and handle errors during strategy execution (e.g. no data for the requested period, division by zero, etc.). Catch exceptions in the backtest logic so they don’t crash the server. Instead, return a clear error message or code (perhaps as a special field in the result or by raising an MCP-friendly error if the SDK supports it). This way, the AI can inform the user what went wrong in a controlled manner.

## MCP Protocol Compliance & Message Handling
- **Use official SDK APIs** – Rely on the MCP Python SDK’s APIs for all message handling. Do not manually construct or parse MCP messages (JSON RPC calls, etc.). Using `FastMCP` and its decorators will ensure that your server speaks in the correct format (for example, listing available tools, handling `listOfferings`, etc. is done for you). This guarantees compatibility with MCP clients like Cursor or others out-of-the-box.
- **Correct use of tools vs resources** – Adhere to MCP’s expectations: use **Tools** for actions (operations that can change state or perform computations) and **Resources** for data retrieval (read-only or observable data). For instance, a `run_backtest` should be a Tool (since it performs a computation and possibly changes internal state), while something like `get_strategy_result` or `fetch_data` could be a Resource if it just returns stored data. Maintaining this distinction ensures the AI agent knows what actions it can take versus data it can query.
- **Message formats and types** – Ensure that every tool/resource function’s inputs and outputs are specified in a way the MCP can understand. Use Python type hints for parameters and return types; the SDK will use these to inform the client of expected types. Return values should be serializable content (e.g. str, int, float, bool, dict, list). If you need to return a complex object (like a custom class), convert it to a dictionary or use the SDK’s content types (if provided, like `TextContent` or others in `mcp.types`) so that it adheres to MCP’s message schema.
- **Observables and subscriptions** – Leverage MCP’s observable pattern for streaming data. If your backtester can provide incremental updates (like per trade or periodic stats), expose a resource that clients can **subscribe** to (the SDK supports `subscribe` on resources). For example, you might have a resource `status://{strategy_id}` that yields live status or results as they update. Use the SDK’s mechanisms (like yielding in an async generator or calling update callbacks) to push new data to subscribers. This aligns with MCP’s `subscribe`/`listChanged` model for resources, rather than keeping long-running requests open on tools.
- **Testing protocol compliance** – After implementing your server, test it with an MCP client (e.g., Cursor’s MCP integration or the MCP Inspector tool) to ensure it properly announces tools/resources and handles calls. The server should correctly respond to discovery calls like listing tools and resources. If you use `FastMCP`, it will automatically handle endpoints like `tools/list` or `resources/list`; just verify that your functions appear with expected names and descriptions. Any mismatch in message format (for example, returning an unsupported type) can cause client-side errors, so catch those early.
- **Consistent content schema** – When sending data back to the client, stick to MCP’s content guidelines. For textual data, return plain strings or use `base.TextContent` if the SDK requires wrapping text. For tabular or structured data, consider returning JSON text or a Python dict (which will be sent as structured JSON). If your action needs to send both a message and a dataset, you might send a descriptive string along with a structured part, or break it into two calls (the design should make it clear to the AI how to consume the result).

## Data Modeling
- **Structured strategy representation** – Define clear data models for strategies. For instance, you might create a `Strategy` class or dataclass that holds strategy parameters (indicators, thresholds, etc.) and perhaps a reference to the strategy’s logic function. This makes it easier to manage strategies (each registered agent could be an instance of `Strategy` with a name or ID). It also adds clarity to the code by avoiding passing around loosely-defined dicts for strategy config.
- **Backtest result model** – Similarly, model the output of a backtest in a structured way. A dataclass like `BacktestResult` could include fields for summary statistics (returns, Sharpe ratio, etc.) and maybe lists of trades or equity curve. Using a model class helps ensure consistency in what you output. When it’s time to send results via MCP, you can convert this class to a dictionary (e.g. via `dataclasses.asdict`) so that it serializes to JSON cleanly.
- **Use schema conventions** – If the MCP ecosystem or your team has conventions for data (for example, using ISO timestamps for dates, or specific key names like `"strategy_id"`, `"timestamp"`, `"price"`), adhere to those in your models. Consistency in field naming and formats will prevent confusion and reduce the chance of errors when the AI or other tools consume the data.
- **Avoid overly complex structures** – Keep the data models for inputs/outputs relatively simple in structure (flat or moderately nested dictionaries). Deeply nested or complex object graphs could be hard for the AI to navigate and for the protocol to transmit. If you have complex internals, distill them into summary results before returning. For example, instead of returning a full pandas DataFrame object, extract the needed info (or send a CSV string or list of records) as the resource output.
- **Document the data model** – Along with coding the models, document what each model contains and how it’s used. This could mean adding docstrings to your data classes or comments in code where the model is produced/consumed. This documentation acts as guidance for contributors and also for the AI model in understanding what data to expect (especially if you include these files in context via `@file` in Cursor rules).

## Testing & Modularity
- **Design for testability** – Write the code so that business logic can be run without the MCP environment. For example, ensure the backtest function can accept parameters (like historical price data, strategy config) and return results, without needing global state. This way, you can unit test the backtest logic easily by feeding in sample data and verifying outputs, making the system more robust.
- **Unit test critical components** – Develop a suite of unit tests for the core logic: strategy signal generation, trade execution, P&L calculation, etc. Each function or class method that implements a piece of trading logic should have corresponding tests (e.g. test that a buy signal is generated under certain conditions, test that P&L is calculated correctly for a given trade list). This will catch bugs early and also implicitly enforce a cleaner design (code that’s hard to test often indicates design problems).
- **Mock external dependencies** – If your backtest pulls data from external sources (files, databases), abstract that behind an interface so it can be easily mocked. For instance, use a `DataFeed` class that you can swap out with a fake implementation in tests. This prevents tests from relying on external I/O and makes them faster and more reliable. It also aligns with clean architecture by inverting dependencies (the backtester depends on a data interface, not a concrete data fetch).
- **Modular project structure** – Organize the project into logical modules or packages. For example: `mcp_server/` for MCP interface code, `strategies/` for strategy definitions, `backtest/` for the backtest engine and related utils, `models/` for data models, etc. This modularity makes it easier to reason about the code and to replace parts. It also helps the AI by providing context-specific files (you can include only relevant modules in prompts, thanks to Cursor’s rules, which keeps context focused).
- **Integration testing** – Beyond unit tests, do integration tests where you simulate an MCP interaction. This could be as simple as calling a tool function directly in a test (since it’s just a Python function) and verifying the side effects or outputs. You can also run the server in a test mode (maybe using `mcp.dev` CLI or a test client from the SDK) and simulate a full backtest request to see if the system behaves as expected (ensuring that registration, execution, and result retrieval flow works end-to-end).

## Naming & Documentation
- **Follow PEP 8 naming** – Use clear and standard Python naming conventions throughout. Functions, methods, and variables should use `snake_case` (e.g. `run_backtest`), classes use `CapWords` (e.g. `BacktestRunner`), and constants are `UPPER_SNAKE_CASE`. Consistent naming makes the codebase easier to navigate and signals to the AI the roles of different symbols.
- **Descriptive names** – Choose names that convey intent. For example, prefer `register_strategy` over a vague name like `add` or `do_strategy`. Similarly, name the MCP tools/resources in a way that the AI (and other developers) can immediately tell what they do (e.g., a tool `run_strategy_backtest` or a resource `get_equity_curve`). This reduces ambiguity when the AI is generating or modifying code.
- **Docstrings and comments** – Every public function, class, and especially each MCP tool/resource should have a docstring explaining its purpose, parameters, and return value. This is crucial not only for developers but also for AI understanding. For instance, a tool’s docstring can describe what the tool does (“Runs the backtest for a given strategy and returns summary results”), which the AI might use when formulating responses or suggestions. Use multiline docstrings (`""" """`) for clarity, and include examples of usage if relevant.
- **Module documentation** – In complex modules (like the backtest engine), consider adding a module-level comment or docstring at the top that outlines how it works. Also update the project’s README or developer guide to explain the high-level architecture (how strategies are registered and executed via MCP). This provides essential context to anyone reading the code or using AI assistance on the project.
- **Consistency in terminology** – Decide on consistent terminology for key concepts (e.g., whether to call it “strategy”, “agent”, or “bot”; “backtest” or “simulation”; “portfolio” or “account”). Use the chosen terms uniformly in code and documentation. Mixing terms can confuse both human readers and the AI model. For example, if you choose the term “strategy”, then your functions might be `register_strategy`, `run_strategy`, `get_strategy_result` rather than mixing in `agent` or `model` in some places.

## Common Pitfalls to Avoid
- **No global mutable state** – Refrain from using module-level global variables to store state like current strategies or results. This can lead to conflicts especially in a multi-session scenario or when running tests in parallel. Instead, encapsulate state within classes or use the MCP session context. For instance, use an in-memory store indexed by session or a field in a Context object passed around. This ensures one user’s strategy data doesn’t accidentally leak into another’s.
- **Blocking calls without yield** – Avoid writing tools that perform long computations or blocking I/O without giving control back. A common pitfall is to loop over a large dataset in a tool function for many seconds, which will block the event loop. Instead, make such functions asynchronous and use `await` for any I/O, or chunk the work and use `asyncio.sleep(0)` or similar techniques to yield occasionally. This keeps the MCP communication responsive. If truly long-running, consider offloading to a separate thread or process, and inform the user that the task is running.
- **Ignoring protocol nuances** – Don’t forget that MCP has specific expectations (e.g., after connecting, the client might automatically call for the list of tools/resources). If using the high-level SDK, it covers these, but if you ever drop to low-level, ensure you handle things like the initial handshake, `listChanged` notifications when tools/resources change, etc. A pitfall would be adding a new tool at runtime without notifying the client – in general, define all tools/resources at startup so they’re known, or use the SDK’s dynamic update features if any.
- **Poor error messaging** – Be careful not to return Python stack traces or generic errors directly to the user/AI. Any exception that occurs in a tool should be caught and turned into a user-friendly message or a handled error. For example, catching an exception and returning `"Error: Strategy failed due to XYZ"` is better than letting a traceback bubble up. Unhandled exceptions might terminate the session or at least confuse the AI. Use logging for debug details on the server side, but send concise error info to the client.
- **Misusing observables** – If implementing streaming data or subscriptions, avoid pushing data in an uncontrolled way. Always use the proper subscription channels. A pitfall would be trying to implement your own threading/timer to send messages outside of MCP’s knowledge. Instead, if the SDK offers a way to update a resource (like calling an update method on a resource or yielding values in an async generator), use that. This ensures the client receives data in the expected format (e.g. as incremental `listChanged` updates for a subscribed resource).
- **Not cleaning up** – Failing to release resources can be an issue, especially if the backtest opens file handles or database connections. Use the server lifespan hooks or context managers to ensure files are closed and connections are dropped after use. Similarly, if you spawn background tasks or threads, make sure they don’t outlive their purpose (perhaps cancel them if the session ends or if the user triggers a stop).
- **Skipping documentation** – A final pitfall is neglecting to document or update the rules and guidelines as the project evolves. If another developer or an AI comes in later, they may not understand the design decisions. Keep these rules up-to-date and refined as you discover what works best. This ensures the AI continues to generate code consistent with the project’s architecture and you avoid regressing on the clean architecture and best practices you've established.